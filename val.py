# llm_eval.py
import os
import json
from datetime import datetime
from tqdm import tqdm
import pandas as pd

# å¤ç”¨ä½ é¡¹ç›®ä¸­å·²æœ‰çš„ openai å®¢æˆ·ç«¯å’Œé…ç½®
from rag_agent import RAGAgent
from config import MODEL_NAME, OPENAI_API_KEY, OPENAI_API_BASE
from openai import OpenAI

class LLMEvaluator:
    def __init__(self):
        # åˆå§‹åŒ–ä½ çš„RAGæ™ºèƒ½ä½“å’Œç”¨äºè¯„ä¼°çš„LLMå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨åŒä¸€ä¸ªï¼‰
        self.rag_agent = RAGAgent(model=MODEL_NAME)
        self.eval_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)

        # ï¼ï¼ï¼æ ¸å¿ƒï¼šè¯·æ›¿æ¢æˆä½ ç²¾å¿ƒè®¾è®¡çš„5-10ä¸ªæµ‹è¯•é—®é¢˜ ï¼ï¼ï¼
        self.test_questions = [
            "æ ¹æ®åº·å¥ˆå°”ï¼ˆConnellï¼‰çš„ç†è®ºï¼Œä»€ä¹ˆæ˜¯â€˜éœ¸æƒç”·æ€§æ°”è´¨â€™ï¼ˆHegemonic masculinityï¼‰ï¼Ÿ",
            "è¯·ç®€è¿°æ€§åˆ«ç§©åºï¼ˆGender Orderï¼‰è¿™ä¸€æ¦‚å¿µçš„æ ¸å¿ƒå†…å®¹ã€‚",
            "äº¤å‰æ€§ï¼ˆIntersectionalityï¼‰è§†è§’å¦‚ä½•å¸®åŠ©æˆ‘ä»¬åˆ†æç¤¾ä¼šä¸å¹³ç­‰ï¼Ÿ",
            "LGBTæ˜¯ä»€ä¹ˆï¼Ÿ",
            "æ ˆå’Œé˜Ÿåˆ—çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
            "ç®€è¿°æŸ¥æ‰¾çš„å‡ ç§æ–¹æ³•",
            "å¦‚ä½•åˆ©ç”¨äºŒå‰æ ‘è¿›è¡Œæ’åºï¼Ÿ",
            # ... è¯·åœ¨æ­¤æ·»åŠ æ›´å¤šåŸºäºä½ è¯¾ç¨‹èµ„æ–™çš„å…·ä½“é—®é¢˜
        ]

    def ask_rag(self, question):
        """è°ƒç”¨ä½ çš„RAGç³»ç»Ÿè·å–ç­”æ¡ˆå’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‚"""
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®ä½  rag_agent.answer_question çš„å®é™…è¿”å›å€¼æ¥è°ƒæ•´
        result = self.rag_agent.answer_question(question)
        answer = result.get("answer", "")
        # å…³é”®ï¼šæå–å‡ºæ£€ç´¢åˆ°çš„åŸå§‹ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œç”¨äºåç»­è¯„ä¼°
        # å‡è®¾ä½ çš„RAGè¿”å›çš„æ£€ç´¢ç»“æœåœ¨ 'retrieved_docs' å­—æ®µä¸­
        contexts = []
        if 'retrieved_docs' in result and result['retrieved_docs']:
            for doc in result['retrieved_docs']:
                contexts.append(doc.get('content', ''))
        # å¦‚æœæ ¼å¼ä¸åŒï¼Œä½ å¯èƒ½éœ€è¦è¿™æ ·è°ƒæ•´ï¼š
        # contexts = [result.get("context", "")]
        return answer, contexts

    def llm_as_judge(self, question, answer, contexts):
        """è®©LLMä½œä¸ºè£åˆ¤ï¼Œå¯¹RAGçš„ç­”æ¡ˆè¿›è¡Œè¯„ä¼°ã€‚"""
        # å°†ä¸Šä¸‹æ–‡æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²
        context_text = "\n---\n".join(contexts)

        # è®¾è®¡è¯„ä¼°æç¤ºè¯ï¼ˆä½ å¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´ç»´åº¦å’Œæ ‡å‡†ï¼‰
        evaluation_prompt = f"""
è¯·ä½ ä½œä¸ºä¸€åä¸¥æ ¼çš„å­¦æœ¯åŠ©æ•™ï¼Œè¯„ä¼°ä»¥ä¸‹é—®ç­”çš„è´¨é‡ã€‚

ã€å­¦ç”Ÿé—®é¢˜ã€‘
{question}

ã€åŠ©æ•™å‚è€ƒçš„è¯¾ç¨‹ææ–™ï¼ˆä¸Šä¸‹æ–‡ï¼‰ã€‘
{context_text if context_text.strip() else 'ï¼ˆæ— ç›¸å…³å†…å®¹ï¼‰'}

ã€åŠ©æ•™ç»™å‡ºçš„ç­”æ¡ˆã€‘
{answer}

è¯·ä»ä»¥ä¸‹ä¸¤ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼Œå¹¶åˆ†åˆ«ç»™å‡º1-5åˆ†çš„æ•´æ•°æ‰“åˆ†ï¼ˆ1åˆ†æœ€å·®ï¼Œ5åˆ†æœ€å¥½ï¼‰ï¼Œä»¥åŠä¸€å¥ç®€çŸ­çš„è¯„è¯­ã€‚

è¯„ä¼°ç»´åº¦ï¼š
1. **å¿ å®åº¦**ï¼šç­”æ¡ˆæ˜¯å¦ä¸¥æ ¼åŸºäºä¸Šæ–¹æä¾›çš„â€œè¯¾ç¨‹ææ–™ï¼ˆä¸Šä¸‹æ–‡ï¼‰â€ï¼Œæ˜¯å¦åŒ…å«æ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ¨æ–­å‡ºçš„ä¿¡æ¯æˆ–â€œå¹»è§‰â€ã€‚
2. **ç›¸å…³åº¦**ï¼šç­”æ¡ˆæ˜¯å¦ç›´æ¥ã€å®Œæ•´åœ°å›åº”äº†â€œå­¦ç”Ÿé—®é¢˜â€ï¼Œæ˜¯å¦ç­”éæ‰€é—®æˆ–é—æ¼å…³é”®ç‚¹ã€‚

è¯·ä»¥ä¸¥æ ¼çš„JSONæ ¼å¼è¾“å‡ºï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "scores": {{
        "faithfulness": ...,
        "relevancy": ...
    }},
    "comments": {{
        "faithfulness": "...",
        "relevancy": "..."
    }}
}}
"""

        try:
            response = self.eval_client.chat.completions.create(
                model=MODEL_NAME,  # ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå…¬æ­£ã€ä¸¥æ ¼çš„è¯„ä¼°è€…ï¼Œæ€»æ˜¯è¾“å‡ºæœ‰æ•ˆçš„JSONã€‚"},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.1,  # ä½æ¸©åº¦ä»¥ä¿è¯è¯„ä¼°ç¨³å®šæ€§
                response_format={"type": "json_object"}  # è¦æ±‚è¿”å›JSON
            )
            evaluation_result = json.loads(response.choices[0].message.content)
            return evaluation_result
        except Exception as e:
            print(f"LLMè¯„ä¼°å‡ºé”™: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤çš„è¯„ä¼°ç»“æœ
            return {
                "scores": {"faithfulness": 0, "relevancy": 0},
                "comments": {"faithfulness": "è¯„ä¼°å¤±è´¥", "relevancy": "è¯„ä¼°å¤±è´¥"}
            }

    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹ã€‚"""
        print("ğŸ§ª å¼€å§‹åŸºäºLLMçš„RAGç³»ç»Ÿè¯„ä¼°...")
        all_results = []

        for question in tqdm(self.test_questions, desc="è¯„ä¼°è¿›åº¦"):
            # 1. RAGç³»ç»Ÿç”Ÿæˆç­”æ¡ˆ
            answer, contexts = self.ask_rag(question)

            # 2. LLMå¯¹ç­”æ¡ˆè¿›è¡Œè¯„ä¼°
            eval_result = self.llm_as_judge(question, answer, contexts)

            # 3. è®°å½•ç»“æœ
            record = {
                "question": question,
                "answer": answer,
                "contexts": contexts,
                "scores": eval_result["scores"],
                "comments": eval_result["comments"]
            }
            all_results.append(record)

        # 4. ä¿å­˜ç»“æœ
        self.save_results(all_results)
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ 'llm_eval_results/' ç›®å½•ã€‚")

    def save_results(self, results):
        """å°†è¯„ä¼°ç»“æœä¿å­˜ä¸ºJSONå’ŒCSVæ–‡ä»¶ã€‚"""
        output_dir = "llm_eval_results"
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜è¯¦ç»†çš„JSONç»“æœ
        json_path = os.path.join(output_dir, f"detailed_results_{timestamp}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        # ä¿å­˜ç®€æ˜çš„CSVåˆ†æ•°æ€»è¡¨
        df_data = []
        for r in results:
            df_data.append({
                "question": r["question"][:100] + "...",  # é—®é¢˜æ‘˜è¦
                "faithfulness_score": r["scores"]["faithfulness"],
                "relevancy_score": r["scores"]["relevancy"],
                "faithfulness_comment": r["comments"]["faithfulness"],
                "relevancy_comment": r["comments"]["relevancy"]
            })
        df = pd.DataFrame(df_data)
        csv_path = os.path.join(output_dir, f"scores_summary_{timestamp}.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        # æ‰“å°å¹³å‡åˆ†
        avg_faith = df["faithfulness_score"].mean()
        avg_relev = df["relevancy_score"].mean()
        print(f"\nğŸ“Š å¹³å‡åˆ†æ•°ï¼ˆæ»¡åˆ†5åˆ†ï¼‰:")
        print(f"  å¿ å®åº¦ (Faithfulness): {avg_faith:.2f}")
        print(f"  ç›¸å…³åº¦ (Relevancy): {avg_relev:.2f}")
        print(f"\nğŸ“ è¯¦ç»†ç»“æœæ–‡ä»¶:")
        print(f"  {json_path}")
        print(f"  {csv_path}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        evaluator = LLMEvaluator()
        evaluator.run_evaluation()
    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")

if __name__ == "__main__":
    main()